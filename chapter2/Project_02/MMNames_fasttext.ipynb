{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd473e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!pip uninstall numpy gensim -y\n",
    "# !pip install numpy gensim --force-reinstall --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a239964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nuwai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nuwai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nuwai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, FastText\n",
    "import pandas as pd\n",
    "import data_preprocessing as dp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0009dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "395f4b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/MMNames_clean.csv')\n",
    "df = dp.clean_name_column(df, 'name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93e34838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 15087, After deduplication: 15087\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify ambiguous names (appear in multiple regions)\n",
    "dupes = df.groupby(\"name\")[\"SR_Name\"].nunique()\n",
    "ambiguous_names = dupes[dupes > 1].index\n",
    "\n",
    "# Step 2: For ambiguous names, keep only the first occurrence\n",
    "ambiguous_df = df[df[\"name\"].isin(ambiguous_names)]\n",
    "ambiguous_deduped = ambiguous_df.drop_duplicates(subset=\"name\", keep=\"first\")\n",
    "\n",
    "# Step 3: For non-ambiguous names, just keep them as-is\n",
    "non_ambiguous_df = df[~df[\"name\"].isin(ambiguous_names)]\n",
    "\n",
    "# Step 4: Combine them back together\n",
    "df = pd.concat([non_ambiguous_df, ambiguous_deduped], ignore_index=True)\n",
    "\n",
    "# Optional: Check final size\n",
    "print(f\"Original: {len(df)}, After deduplication: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20465bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(df['SR_Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f7caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume df['name'] contains romanized text\n",
    "df['name'].dropna().astype(str).to_csv(\"names.txt\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0dd565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each name into a list of characters or subwords (or words, depending on data)\n",
    "df['tokens'] = df['name'].astype(str).apply(lambda x: x.split())  # word-level\n",
    "# OR for character-level: list(x)\n",
    "tokenized_data = df['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33d0937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_burmese_phonics(text):\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # Mapping of Romanized Burmese phonics to standard forms\n",
    "    phonics_map = {\n",
    "        # Aspirated consonants → base form\n",
    "        'ph': 'p',\n",
    "        'hp': 'p',\n",
    "        'hpy': 'py',\n",
    "        'hs': 's',\n",
    "        'th': 't',\n",
    "        'ht': 't',\n",
    "        'kh': 'k',\n",
    "        'hk': 'k',\n",
    "        'ng': 'n',\n",
    "        'ny': 'n',\n",
    "        'my': 'm',\n",
    "\n",
    "        # Diphthongs and vowels\n",
    "        'oo': 'u',\n",
    "        'ou': 'u',\n",
    "        'au': 'o',\n",
    "        'aw': 'o',\n",
    "        'ae': 'e',\n",
    "        'ay': 'e',\n",
    "        'ei': 'e',\n",
    "        'ia': 'ya',   # ex: \"Pyi A\" or \"Pya\"\n",
    "        'ua': 'wa',\n",
    "\n",
    "        # Word endings or tones\n",
    "        'aung': 'ong',\n",
    "        'auk': 'ok',\n",
    "        'ein': 'en',\n",
    "        'yin': 'in',\n",
    "        'yan': 'an',\n",
    "\n",
    "        # Optional tone reduction\n",
    "        'ya': 'a',\n",
    "        'wa': 'a',\n",
    "        'ra': 'a',\n",
    "\n",
    "        # Silent or redundant\n",
    "        'rr': 'r',\n",
    "        'll': 'l',\n",
    "        'pp': 'p',\n",
    "        'tt': 't',\n",
    "        'kk': 'k',\n",
    "        'mm': 'm',\n",
    "        'nn': 'n',\n",
    "        'gg': 'g',\n",
    "        'ss': 's',\n",
    "    }\n",
    "\n",
    "    # Apply rules based on length, avoiding overlap\n",
    "    for k in sorted(phonics_map, key=lambda x: -len(x)):\n",
    "        text = re.sub(k, phonics_map[k], text)\n",
    "\n",
    "    # Remove unwanted characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    # Normalize whitespace and repeated letters\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "629800f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name'] = df['name'].apply(normalize_burmese_phonics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1b358b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              bogale\n",
       "1             danubyu\n",
       "2                dede\n",
       "3                enme\n",
       "4             hintada\n",
       "             ...     \n",
       "15082          nar ku\n",
       "15083     tone bo gyi\n",
       "15084    pan kar kone\n",
       "15085         par kar\n",
       "15086          an mai\n",
       "Name: name, Length: 15087, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfe7dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a27a4e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import FastText\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Bidirectional, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# === PARAMETERS ===\n",
    "MAX_LEN = 30\n",
    "EMBEDDING_DIM = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "TEST_SIZE = 0.3\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# === 1. Tokenize names into char lists for FastText ===\n",
    "df['tokens'] = df['name'].astype(str).apply(lambda x: list(x.strip().lower()))\n",
    "tokenized_data = df['tokens'].tolist()\n",
    "\n",
    "# === 2. Train FastText model on all data (unsupervised embeddings) ===\n",
    "fasttext_model = FastText(\n",
    "    sentences=tokenized_data,\n",
    "    vector_size=EMBEDDING_DIM,\n",
    "    window=3,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    sg=1  # skip-gram\n",
    ")\n",
    "fasttext_model.save(\"fasttext_gensim.model\")\n",
    "\n",
    "# === 3. Prepare texts for Keras Tokenizer (space-separated chars) ===\n",
    "texts_for_keras = [' '.join(tokens) for tokens in tokenized_data]\n",
    "\n",
    "# === 4. Split data into train and test BEFORE fitting tokenizer and label encoder to avoid leakage ===\n",
    "X_train_texts, X_test_texts, y_train_raw, y_test_raw = train_test_split(\n",
    "    texts_for_keras, df['SR_Name'], test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=df['SR_Name']\n",
    ")\n",
    "\n",
    "# === 5. Fit Keras Tokenizer ONLY on training data ===\n",
    "tokenizer = Tokenizer(char_level=False, lower=True)  # words = chars separated by space\n",
    "tokenizer.fit_on_texts(X_train_texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "# === 6. Encode labels with LabelEncoder fitted on train labels ONLY ===\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train_raw)\n",
    "\n",
    "y_train = label_encoder.transform(y_train_raw)\n",
    "y_test = label_encoder.transform(y_test_raw)\n",
    "\n",
    "# === 7. Create embedding matrix from FastText for tokenizer vocabulary ===\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in fasttext_model.wv:\n",
    "        embedding_matrix[i] = fasttext_model.wv[word]\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.normal(size=(EMBEDDING_DIM,))\n",
    "\n",
    "# === 8. Convert texts to padded sequences ===\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_texts)\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_texts)\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c3d2c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.4572616263964666, 1: 1.178045515394913, 2: 1.4593698175787728, 3: 1.3008130081300813, 4: 1.0628019323671498, 5: 3.963963963963964, 6: 1.7617617617617618, 7: 0.5155243116578794, 8: 0.64968623108158, 9: 1.8863879957127545, 10: 6.111111111111111, 11: 0.7770419426048565, 12: 0.3763096001710498, 13: 3.4108527131782944, 14: 0.6411657559198543, 15: 1.7777777777777777, 16: 1.8863879957127545, 17: 1.455748552522746}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "print(class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad0dcdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization, SpatialDropout1D\n",
    "\n",
    "\n",
    "def create_conv_lstm_model(vocab_size, max_len, num_classes,embedding_matrix):\n",
    "    model = Sequential([\n",
    "        Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_matrix.shape[1],\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=max_len,\n",
    "            trainable=False  # freeze embeddings, set True if you want to fine-tune\n",
    "        ),\n",
    "        SpatialDropout1D(0.2),\n",
    "        Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Bidirectional(LSTM(64, return_sequences=True)),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f84d411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.0622 - loss: 2.8828 - val_accuracy: 0.0269 - val_loss: 2.8675\n",
      "Epoch 2/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.0732 - loss: 2.8294 - val_accuracy: 0.0870 - val_loss: 2.7647\n",
      "Epoch 3/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.0860 - loss: 2.7441 - val_accuracy: 0.0568 - val_loss: 2.7394\n",
      "Epoch 4/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.0988 - loss: 2.6838 - val_accuracy: 0.1582 - val_loss: 2.6516\n",
      "Epoch 5/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.1175 - loss: 2.6141 - val_accuracy: 0.1456 - val_loss: 2.6490\n",
      "Epoch 6/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.1181 - loss: 2.5988 - val_accuracy: 0.1127 - val_loss: 2.6309\n",
      "Epoch 7/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.1316 - loss: 2.5294 - val_accuracy: 0.1522 - val_loss: 2.6019\n",
      "Epoch 8/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.1458 - loss: 2.5370 - val_accuracy: 0.1398 - val_loss: 2.5869\n",
      "Epoch 9/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.1490 - loss: 2.4186 - val_accuracy: 0.1575 - val_loss: 2.6098\n",
      "Epoch 10/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.1533 - loss: 2.4354 - val_accuracy: 0.1608 - val_loss: 2.5853\n",
      "Epoch 11/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.1687 - loss: 2.3716 - val_accuracy: 0.1599 - val_loss: 2.5439\n",
      "Epoch 12/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.1830 - loss: 2.3386 - val_accuracy: 0.1677 - val_loss: 2.5115\n",
      "Epoch 13/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.1856 - loss: 2.2594 - val_accuracy: 0.1816 - val_loss: 2.4897\n",
      "Epoch 14/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.2044 - loss: 2.1954 - val_accuracy: 0.1714 - val_loss: 2.5122\n",
      "Epoch 15/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.1954 - loss: 2.1951 - val_accuracy: 0.1694 - val_loss: 2.5161\n",
      "Epoch 16/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.2041 - loss: 2.1472 - val_accuracy: 0.1694 - val_loss: 2.5236\n",
      "Epoch 17/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.2137 - loss: 2.1244 - val_accuracy: 0.1911 - val_loss: 2.4753\n",
      "Epoch 18/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.2288 - loss: 2.0351 - val_accuracy: 0.1785 - val_loss: 2.5250\n",
      "Epoch 19/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.2224 - loss: 1.9957 - val_accuracy: 0.1856 - val_loss: 2.5069\n",
      "Epoch 20/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.2359 - loss: 1.9617 - val_accuracy: 0.1981 - val_loss: 2.4617\n",
      "Epoch 21/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.2469 - loss: 1.9210 - val_accuracy: 0.1977 - val_loss: 2.4707\n",
      "Epoch 22/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.2642 - loss: 1.8917 - val_accuracy: 0.1886 - val_loss: 2.4981\n",
      "Epoch 23/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - accuracy: 0.2640 - loss: 1.8574 - val_accuracy: 0.1906 - val_loss: 2.5226\n",
      "Epoch 24/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.2652 - loss: 1.8334 - val_accuracy: 0.2017 - val_loss: 2.5129\n",
      "Epoch 25/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.2758 - loss: 1.7810 - val_accuracy: 0.1913 - val_loss: 2.5543\n",
      "Epoch 26/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.2708 - loss: 1.7647 - val_accuracy: 0.2034 - val_loss: 2.5067\n",
      "Epoch 27/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.2783 - loss: 1.7591 - val_accuracy: 0.2180 - val_loss: 2.4920\n",
      "Epoch 28/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.2957 - loss: 1.7184 - val_accuracy: 0.2110 - val_loss: 2.5118\n",
      "Epoch 29/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.2925 - loss: 1.6628 - val_accuracy: 0.1959 - val_loss: 2.5114\n",
      "Epoch 30/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.3047 - loss: 1.6437 - val_accuracy: 0.2132 - val_loss: 2.4956\n",
      "Epoch 31/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - accuracy: 0.3116 - loss: 1.6434 - val_accuracy: 0.2114 - val_loss: 2.5057\n",
      "Epoch 32/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.3132 - loss: 1.6203 - val_accuracy: 0.2114 - val_loss: 2.5155\n",
      "Epoch 33/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.3192 - loss: 1.6004 - val_accuracy: 0.2178 - val_loss: 2.5195\n",
      "Epoch 34/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.3272 - loss: 1.5797 - val_accuracy: 0.2306 - val_loss: 2.5176\n",
      "Epoch 35/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.3344 - loss: 1.5518 - val_accuracy: 0.2242 - val_loss: 2.5174\n",
      "Epoch 36/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.3269 - loss: 1.5474 - val_accuracy: 0.2273 - val_loss: 2.5058\n",
      "Epoch 37/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - accuracy: 0.3452 - loss: 1.5044 - val_accuracy: 0.2160 - val_loss: 2.5505\n",
      "Epoch 38/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.3369 - loss: 1.4989 - val_accuracy: 0.2333 - val_loss: 2.5669\n",
      "Epoch 39/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.3588 - loss: 1.4594 - val_accuracy: 0.2216 - val_loss: 2.5683\n",
      "Epoch 40/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.3438 - loss: 1.4242 - val_accuracy: 0.2284 - val_loss: 2.5725\n",
      "Epoch 41/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.3603 - loss: 1.4090 - val_accuracy: 0.2333 - val_loss: 2.5712\n",
      "Epoch 42/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.3735 - loss: 1.3879 - val_accuracy: 0.2313 - val_loss: 2.5456\n",
      "Epoch 43/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.3624 - loss: 1.3980 - val_accuracy: 0.2364 - val_loss: 2.6163\n",
      "Epoch 44/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.3688 - loss: 1.4087 - val_accuracy: 0.2315 - val_loss: 2.6741\n",
      "Epoch 45/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.3696 - loss: 1.3679 - val_accuracy: 0.2293 - val_loss: 2.6070\n",
      "Epoch 46/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.3632 - loss: 1.3953 - val_accuracy: 0.2337 - val_loss: 2.6306\n",
      "Epoch 47/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.3794 - loss: 1.3594 - val_accuracy: 0.2324 - val_loss: 2.6122\n",
      "Epoch 48/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.3823 - loss: 1.3282 - val_accuracy: 0.2311 - val_loss: 2.6271\n",
      "Epoch 49/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.3966 - loss: 1.2968 - val_accuracy: 0.2359 - val_loss: 2.6468\n",
      "Epoch 50/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.3821 - loss: 1.2906 - val_accuracy: 0.2359 - val_loss: 2.6650\n",
      "Epoch 51/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.3929 - loss: 1.3211 - val_accuracy: 0.2381 - val_loss: 2.6745\n",
      "Epoch 52/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.4043 - loss: 1.2598 - val_accuracy: 0.2375 - val_loss: 2.7082\n",
      "Epoch 53/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.4026 - loss: 1.2688 - val_accuracy: 0.2403 - val_loss: 2.7099\n",
      "Epoch 54/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.4095 - loss: 1.2422 - val_accuracy: 0.2368 - val_loss: 2.6995\n",
      "Epoch 55/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.4041 - loss: 1.2617 - val_accuracy: 0.2441 - val_loss: 2.7175\n",
      "Epoch 56/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.4129 - loss: 1.2171 - val_accuracy: 0.2412 - val_loss: 2.7367\n",
      "Epoch 57/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.4158 - loss: 1.1906 - val_accuracy: 0.2414 - val_loss: 2.7251\n",
      "Epoch 58/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.4129 - loss: 1.2484 - val_accuracy: 0.2432 - val_loss: 2.7353\n",
      "Epoch 59/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.4262 - loss: 1.1980 - val_accuracy: 0.2317 - val_loss: 2.7795\n",
      "Epoch 60/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.4168 - loss: 1.2089 - val_accuracy: 0.2485 - val_loss: 2.7406\n",
      "Epoch 61/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.4221 - loss: 1.1838 - val_accuracy: 0.2485 - val_loss: 2.7616\n",
      "Epoch 62/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.4264 - loss: 1.2047 - val_accuracy: 0.2445 - val_loss: 2.7850\n",
      "Epoch 63/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.4267 - loss: 1.1702 - val_accuracy: 0.2463 - val_loss: 2.8333\n",
      "Epoch 64/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.4317 - loss: 1.1690 - val_accuracy: 0.2549 - val_loss: 2.7884\n",
      "Epoch 65/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.4422 - loss: 1.1514 - val_accuracy: 0.2554 - val_loss: 2.7867\n",
      "Epoch 66/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.4327 - loss: 1.1353 - val_accuracy: 0.2478 - val_loss: 2.8250\n",
      "Epoch 67/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.4455 - loss: 1.1284 - val_accuracy: 0.2507 - val_loss: 2.8279\n",
      "Epoch 68/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.4493 - loss: 1.1090 - val_accuracy: 0.2428 - val_loss: 2.8810\n",
      "Epoch 69/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.4426 - loss: 1.0922 - val_accuracy: 0.2569 - val_loss: 2.8586\n",
      "Epoch 70/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.4667 - loss: 1.0612 - val_accuracy: 0.2560 - val_loss: 2.8603\n",
      "Epoch 71/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.4591 - loss: 1.0840 - val_accuracy: 0.2470 - val_loss: 2.8128\n",
      "Epoch 72/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.4553 - loss: 1.1060 - val_accuracy: 0.2490 - val_loss: 2.8366\n",
      "Epoch 73/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.4602 - loss: 1.1060 - val_accuracy: 0.2490 - val_loss: 2.8902\n",
      "Epoch 74/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.4655 - loss: 1.0599 - val_accuracy: 0.2492 - val_loss: 2.9030\n",
      "Epoch 75/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.4544 - loss: 1.0785 - val_accuracy: 0.2571 - val_loss: 2.8855\n",
      "Epoch 76/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.4751 - loss: 1.0501 - val_accuracy: 0.2536 - val_loss: 2.8863\n",
      "Epoch 77/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.4740 - loss: 1.0220 - val_accuracy: 0.2604 - val_loss: 2.9056\n",
      "Epoch 78/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.4605 - loss: 1.0599 - val_accuracy: 0.2509 - val_loss: 2.9049\n",
      "Epoch 79/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.4601 - loss: 1.0855 - val_accuracy: 0.2624 - val_loss: 2.8998\n",
      "Epoch 80/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.4802 - loss: 1.0298 - val_accuracy: 0.2554 - val_loss: 2.9227\n",
      "Epoch 81/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.4810 - loss: 1.0042 - val_accuracy: 0.2582 - val_loss: 2.9475\n",
      "Epoch 82/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.4776 - loss: 1.0134 - val_accuracy: 0.2604 - val_loss: 2.9402\n",
      "Epoch 83/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.4870 - loss: 1.0099 - val_accuracy: 0.2615 - val_loss: 2.9176\n",
      "Epoch 84/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.4836 - loss: 1.0091 - val_accuracy: 0.2697 - val_loss: 2.9668\n",
      "Epoch 85/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.4826 - loss: 0.9866 - val_accuracy: 0.2602 - val_loss: 3.0513\n",
      "Epoch 86/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.4953 - loss: 0.9742 - val_accuracy: 0.2626 - val_loss: 2.9904\n",
      "Epoch 87/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.4912 - loss: 0.9738 - val_accuracy: 0.2635 - val_loss: 2.9631\n",
      "Epoch 88/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.4875 - loss: 0.9799 - val_accuracy: 0.2633 - val_loss: 2.9973\n",
      "Epoch 89/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.4869 - loss: 0.9667 - val_accuracy: 0.2573 - val_loss: 3.0207\n",
      "Epoch 90/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.4871 - loss: 1.0012 - val_accuracy: 0.2593 - val_loss: 3.0843\n",
      "Epoch 91/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.5007 - loss: 0.9874 - val_accuracy: 0.2646 - val_loss: 3.0043\n",
      "Epoch 92/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.5007 - loss: 0.9826 - val_accuracy: 0.2737 - val_loss: 2.9594\n",
      "Epoch 93/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.4988 - loss: 0.9795 - val_accuracy: 0.2620 - val_loss: 3.0421\n",
      "Epoch 94/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.5081 - loss: 0.9464 - val_accuracy: 0.2589 - val_loss: 2.9867\n",
      "Epoch 95/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.5038 - loss: 0.9514 - val_accuracy: 0.2660 - val_loss: 3.0877\n",
      "Epoch 96/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.5058 - loss: 0.9165 - val_accuracy: 0.2746 - val_loss: 3.1041\n",
      "Epoch 97/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.5089 - loss: 0.9490 - val_accuracy: 0.2600 - val_loss: 3.0567\n",
      "Epoch 98/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.5034 - loss: 0.9507 - val_accuracy: 0.2724 - val_loss: 3.0959\n",
      "Epoch 99/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.5172 - loss: 0.9170 - val_accuracy: 0.2726 - val_loss: 3.1396\n",
      "Epoch 100/100\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.5124 - loss: 0.9326 - val_accuracy: 0.2657 - val_loss: 3.0691\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.2730 - loss: 3.0026\n",
      "Test loss: 3.0691, Test accuracy: 0.2657\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "\n",
    "def create_wide_conv_model(vocab_size, max_len, num_classes, embedding_matrix): \n",
    "    model = Sequential([\n",
    "        Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_matrix.shape[1],\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=max_len,\n",
    "            trainable=False  # freeze embeddings, set True if you want to fine-tune\n",
    "        ),\n",
    "        Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dropout(0.3 ),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model  \n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = create_conv_lstm_model(vocab_size, MAX_LEN, num_classes, embedding_matrix)\n",
    "#model = create_wide_conv_model(vocab_size, MAX_LEN, num_classes, embedding_matrix)\n",
    "\n",
    "# === 10. Setup callbacks ===\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# === 11. Train model with validation on test set ===\n",
    "history = model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_test_padded, y_test),  class_weight=class_weights_dict,\n",
    "    #callbacks=[early_stop, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# === 12. Evaluate on test set ===\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test, verbose=1)\n",
    "print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f8e7742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9257 - loss: 0.2559\n",
      "Test loss: 0.2578, Train accuracy: 0.9255\n"
     ]
    }
   ],
   "source": [
    "# === 12. Evaluate on test set ===\n",
    "loss, accuracy = model.evaluate(X_train_padded, y_train, verbose=1)\n",
    "print(f\"Test loss: {loss:.4f}, Train accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49bc9736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    top_k_accuracy_score\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_classification_model(model, X, y_true, output_path, prefix=\"test\", batch_size=32, top_k=3, label_encoder=None):\n",
    "    # Predict probabilities\n",
    "    y_probs = model.predict(X, batch_size=batch_size, verbose=0)\n",
    "    y_pred = np.argmax(y_probs, axis=1)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Top-k Accuracy (optional)\n",
    "    top_k_acc = top_k_accuracy_score(y_true, y_probs, k=top_k)\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(y_true, y_pred, output_dict=True,target_names=le.classes_)\n",
    "    report_df = pd.DataFrame(report).round(2).transpose()\n",
    "    report_df.loc[\"accuracy\"] = acc\n",
    "    report_df.loc[f\"top_{top_k}_accuracy\"] = top_k_acc\n",
    "\n",
    "    # Save Report\n",
    "    report_df.to_csv(f\"{output_path}/cls_report_{prefix}.csv\", index=True)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    if label_encoder is not None:\n",
    "        xticks = yticks = label_encoder.classes_\n",
    "        label_map = dict(enumerate(label_encoder.classes_))\n",
    "    else:\n",
    "        xticks = yticks = np.arange(len(np.unique(y_true)))\n",
    "        label_map = None\n",
    "    print(f\"label map: {label_map}\")\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=xticks, yticklabels=yticks)\n",
    "    plt.title(f\"Confusion Matrix - {prefix}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_path}/confusion_matrix_{prefix}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        f\"top_{top_k}_accuracy\": top_k_acc,\n",
    "        \"classification_report\": report_df,\n",
    "        \"confusion_matrix\": cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58a0f602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label map: None\n",
      "label map: None\n",
      "0.3565275016567263\n",
      "0.9204545454545454\n"
     ]
    }
   ],
   "source": [
    "test_results = evaluate_classification_model(model, X_test_padded, y_test, './data', prefix=\"_norm_test_wideconv_fastext_epoch80\")\n",
    "train_results = evaluate_classification_model(model, X_train_padded, y_train, './data', prefix=\"norm_train_convlstm_fasttext_epoch80\")\n",
    "print(test_results['accuracy'])\n",
    "print(train_results['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
